{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b783c2c1-6924-45c5-8fdb-8a253e3e5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "torch.cuda.set_device(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce73ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dcb392c-d4df-4b4d-aff6-bd0724da133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        overall_train_data = []\n",
    "        overall_test_data = []\n",
    "        y_train = []\n",
    "        y_test = []\n",
    "        subfolders = {os.path.dirname(name) for name in zip_ref.namelist()}\n",
    "        \n",
    "        for subfolder in subfolders:\n",
    "            if subfolder in ['dataset', 'dataset/z', 'dataset/y'] :\n",
    "                continue\n",
    "            files_in_subfolder = [name for name in zip_ref.namelist() if name.startswith(subfolder) and not name.endswith('.directory')]\n",
    "            class_name = subfolder.split('/')[-1]\n",
    "            train_files, test_files = train_test_split(files_in_subfolder, train_size=0.8)\n",
    "            overall_train_data += train_files\n",
    "            overall_test_data += test_files\n",
    "            y_train += [class_name] * len(train_files)\n",
    "            y_test += [class_name] * len(test_files)\n",
    "\n",
    "        return overall_train_data, overall_test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cd07ba-8a05-4991-b43e-84401060565d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/MoreSymbol.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/MoreSymbol.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mextract_and_split\u001b[0;34m(zip_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_and_split\u001b[39m(zip_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      3\u001b[0m         overall_train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m         overall_test_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.11/zipfile.py:1281\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/MoreSymbol.zip'"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = extract_and_split('./MoreSymbol.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e764ff-98ab-4b9d-8191-4c68d555516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and find unique classes\n",
    "all_classes = np.unique(y_train)\n",
    "\n",
    "# Create a mapping dictionary\n",
    "mapping = {class_name.item(): idx for idx, class_name in enumerate(all_classes)}\n",
    "\n",
    "# Apply the mapping\n",
    "y_train = torch.tensor([mapping[class_name] for class_name in y_train])\n",
    "y_test = torch.tensor([mapping[class_name] for class_name in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d77912-f457-4ad5-b35d-dc4fb16dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(range(len(x_train)))\n",
    "x_train = np.array(x_train)[perm]\n",
    "y_train = np.array(y_train)[perm] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158133c6-5679-48df-940f-d2562583a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 21:53:00.303908: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 21:53:00.358725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential, load_model\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "# from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d16d82-e202-441f-bbcd-a24aef6fbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_list, labels, transforms, zipfile):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.img_list = img_list\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        self.zipfile = zipfile\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_list[idx]\n",
    "        img = Image.open(self.zipfile.open(img)).convert('L')\n",
    "        return self.transforms(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e36a4d-c58b-4324-91d2-6c54f0e64f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 135)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgzip = zipfile.ZipFile(\"/home/liuy29/Image Rec/data/MoreSymbol.zip\")\n",
    "img_path = x_train[456]\n",
    "image = Image.open(imgzip.open(img_path))\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d9f029-487e-48ae-a59a-32d926e020ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=155x135>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dbd06b1-ea7b-4a62-8f42-2bcc67523f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 96493143.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 50854615.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 28298006.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 31645396.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "# train_dataset = MyDataset(x_train, y_train, transform, imgzip)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_dataset = MyDataset(x_test, y_test, transform, imgzip)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80cb142e-ac50-4260-b5b9-bc91bae42655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(np.unique(y_train))\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b583ec-b9c6-4ba2-a454-192be4d5128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_channels = 1\n",
    "conv_layers = nn.Sequential(\n",
    "    nn.Conv2d(input_channels, 16, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, padding=1),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Calculate the size of the output of the conv_layers by doing one forward pass\n",
    "dummy_input = torch.randn(1, input_channels, 100, 100)\n",
    "output = conv_layers(dummy_input)\n",
    "conv_out = output.shape[1] * output.shape[2] * output.shape[3]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    conv_layers,\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(conv_out, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "835fd8a3-0a75-4bc0-b4a2-bf093f122cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69c64e-c9d2-41c4-9570-fb9855ec991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1503724/1773056581.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 4.0793\n",
      "Epoch [2/10], Train Loss: 2.3023\n",
      "Epoch [3/10], Train Loss: 2.3022\n",
      "Epoch [4/10], Train Loss: 2.3020\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        #print('labels', labels)\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    # model.eval()\n",
    "    # total_valid_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for inputs, labels in valid_loader:\n",
    "    #         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    #         # Forward pass\n",
    "    #         outputs = model(inputs)\n",
    "    #         loss = loss_fn(outputs, labels)\n",
    "\n",
    "    #         total_valid_loss += loss.item()\n",
    "\n",
    "    # avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b9fda2-4a00-4f6d-9e58-c1a5ef103c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1503724/1784268762.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8268, Test Accuracy: 0.0689\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert outputs probabilities to predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Calculate correct predictions\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = total_correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164a637-63e4-4d11-9157-e4f42006e4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
